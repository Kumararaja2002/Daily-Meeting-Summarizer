{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e329785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import streamlit as st\n",
    "import docx2txt\n",
    "from dotenv import load_dotenv\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# ====== Load .env ======\n",
    "load_dotenv()\n",
    "os.environ['GROQ_API_KEY'] = os.getenv('GROQ_API_KEY')\n",
    "GROQ_API_KEY = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "# ====== CONFIG ======\n",
    "EXCEL_FILE = \"Meeting_summary_template.xlsx\"\n",
    "MAX_WORKERS = 5  # parallel requests to Groq\n",
    "\n",
    "# ====== JSON SCHEMA PROMPT ======\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are a meeting summarizer. \n",
    "Return the result ONLY in strict JSON following this schema, without extra text:\n",
    "\n",
    "{\n",
    "  \"MeetingDetails\": {\n",
    "    \"Date & Time\": \"\",\n",
    "    \"Location\": \"\",\n",
    "    \"Participants\": []\n",
    "  },\n",
    "  \"Objective\": \"\",\n",
    "  \"AgendaItems\": [],\n",
    "  \"KeyDiscussions\": \"\",\n",
    "  \"DecisionsMade\": \"\",\n",
    "  \"ActionItems\": [\n",
    "    {\"Task\": \"\", \"Owner\": \"\", \"DueDate\": \"\"}\n",
    "  ],\n",
    "  \"NextSteps\": \"\",\n",
    "  \"AdditionalNotes\": \"\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# ====== Helpers ======\n",
    "def read_docx(file_path):\n",
    "    \"\"\"Fast text extraction from .docx\"\"\"\n",
    "    return docx2txt.process(file_path)\n",
    "\n",
    "def call_groq_api(text, force_schema=True):\n",
    "    \"\"\"Call Groq API for summarization\"\"\"\n",
    "    url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "    headers = {\"Authorization\": f\"Bearer {GROQ_API_KEY}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"llama-3.1-8b-instant\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a meeting summarizer.\"},\n",
    "            {\"role\": \"user\", \"content\": PROMPT_TEMPLATE + \"\\n\\nTranscript:\\n\" + text}\n",
    "        ],\n",
    "        \"temperature\": 0.2\n",
    "    }\n",
    "    if force_schema:\n",
    "        payload[\"response_format\"] = {\"type\": \"json_object\"}\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "    result = response.json()\n",
    "    print(result)\n",
    "    return json.loads(result[\"choices\"][0][\"message\"][\"content\"]) if force_schema else result[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5e565ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=4000, chunk_overlap=200):\n",
    "    \"\"\"Split long transcript into manageable chunks\"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "def summarize_chunks_parallel(chunks):\n",
    "    \"\"\"Summarize chunks in parallel and stream results\"\"\"\n",
    "    results = [None] * len(chunks)\n",
    "    progress = st.progress(0, text=\"Starting summarization...\")\n",
    "    output_area = st.container()\n",
    "\n",
    "    def process_chunk(idx, chunk):\n",
    "        return idx, call_groq_api(chunk, force_schema=False)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        futures = [executor.submit(process_chunk, i, chunk) for i, chunk in enumerate(chunks)]\n",
    "        done_count = 0\n",
    "        for future in as_completed(futures):\n",
    "            idx, summary = future.result()\n",
    "            results[idx] = summary\n",
    "            done_count += 1\n",
    "            progress.progress(done_count / len(chunks), text=f\"Processed {done_count}/{len(chunks)} chunks\")\n",
    "            with output_area:\n",
    "                st.markdown(f\"âœ… **Chunk {idx+1} summary (partial):**\")\n",
    "                st.write(summary[:500] + \"...\" if len(summary) > 500 else summary)\n",
    "\n",
    "    return results\n",
    "\n",
    "def merge_summaries(partial_summaries):\n",
    "    \"\"\"Run final summarization pass to produce structured JSON\"\"\"\n",
    "    combined_text = \"\\n\".join([str(s) for s in partial_summaries])\n",
    "    final_summary = call_groq_api(combined_text, force_schema=True)\n",
    "    return final_summary\n",
    "\n",
    "def flatten_summary(summary):\n",
    "    \"\"\"Flatten JSON summary into a row dict for Excel\"\"\"\n",
    "    row = {\n",
    "        \"Meeting_DateTime\": summary[\"MeetingDetails\"].get(\"Date & Time\", \"\"),\n",
    "        \"Meeting_Location\": summary[\"MeetingDetails\"].get(\"Location\", \"\"),\n",
    "        \"Meeting_Participants\": \", \".join(summary[\"MeetingDetails\"].get(\"Participants\", [])),\n",
    "        \"Objective\": summary.get(\"Objective\", \"\"),\n",
    "        \"AgendaItems\": \"; \".join(summary.get(\"AgendaItems\", [])),\n",
    "        \"KeyDiscussions\": summary.get(\"KeyDiscussions\", \"\"),\n",
    "        \"DecisionsMade\": summary.get(\"DecisionsMade\", \"\"),\n",
    "        \"NextSteps\": summary.get(\"NextSteps\", \"\"),\n",
    "        \"AdditionalNotes\": summary.get(\"AdditionalNotes\", \"\")\n",
    "    }\n",
    "\n",
    "    # Handle multiple Action Items\n",
    "    if \"ActionItems\" in summary and summary[\"ActionItems\"]:\n",
    "        for i, action in enumerate(summary[\"ActionItems\"], start=1):\n",
    "            row[f\"ActionItem_{i}_Task\"] = action.get(\"Task\", \"\")\n",
    "            row[f\"ActionItem_{i}_Owner\"] = action.get(\"Owner\", \"\")\n",
    "            row[f\"ActionItem_{i}_DueDate\"] = action.get(\"DueDate\", \"\")\n",
    "    return row\n",
    "\n",
    "def save_to_excel(row):\n",
    "    \"\"\"Append row to Excel file\"\"\"\n",
    "    if os.path.exists(EXCEL_FILE):\n",
    "        df = pd.read_excel(EXCEL_FILE)\n",
    "        df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n",
    "    else:\n",
    "        df = pd.DataFrame([row])\n",
    "\n",
    "    df.to_excel(EXCEL_FILE, index=False)\n",
    "    return df\n",
    "\n",
    "# # ====== Streamlit UI ======\n",
    "# st.set_page_config(page_title=\"Meeting Summarizer\", layout=\"wide\")\n",
    "# st.title(\"ðŸ“‹ Meeting Transcript Summarizer (Parallel + Streaming)\")\n",
    "\n",
    "# uploaded_file = st.file_uploader(\"Upload a transcript (.docx)\", type=[\"docx\"])\n",
    "\n",
    "# if uploaded_file:\n",
    "#     with open(\"temp.docx\", \"wb\") as f:\n",
    "#         f.write(uploaded_file.read())\n",
    "\n",
    "#     transcript = read_docx(\"temp.docx\")\n",
    "#     st.success(f\"âœ… Transcript uploaded successfully! Extracted {len(transcript)//1024} KB of text\")\n",
    "\n",
    "#     with st.spinner(\"Summarizing meeting...\"):\n",
    "#         chunks = chunk_text(transcript)\n",
    "#         st.info(f\"Transcript split into {len(chunks)} chunks\")\n",
    "#         partial_summaries = summarize_chunks_parallel(chunks)\n",
    "#         final_summary = merge_summaries(partial_summaries)\n",
    "#         row = flatten_summary(final_summary)\n",
    "#         df = save_to_excel(row)\n",
    "\n",
    "#     st.success(\"âœ… Final structured summary added to Excel!\")\n",
    "\n",
    "#     # Show last 3 rows with highlight\n",
    "#     st.subheader(\"ðŸ“Š Recent Meeting Summaries\")\n",
    "#     recent_df = df.tail(3)\n",
    "\n",
    "#     def highlight_last_row(x):\n",
    "#         df_styled = pd.DataFrame('', index=x.index, columns=x.columns)\n",
    "#         df_styled.iloc[-1, :] = 'background-color: lightgreen; font-weight: bold;'\n",
    "#         return df_styled\n",
    "\n",
    "#     styled_df = recent_df.style.apply(highlight_last_row, axis=None)\n",
    "#     st.dataframe(styled_df, use_container_width=True, height=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274a3255",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clean_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
